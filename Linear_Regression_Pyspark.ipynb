{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting Spark Session as sc after calling init() of findspark\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# creating spark as pyspark object\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Sample_dataset.csv file from same working directory\n",
    "\n",
    "data = spark.read.csv('sample_dataset.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mpg: double (nullable = true)\n",
      " |-- cylinders: integer (nullable = true)\n",
      " |-- displacement: double (nullable = true)\n",
      " |-- horsepower: integer (nullable = true)\n",
      " |-- weight: integer (nullable = true)\n",
      " |-- acceleration: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# looking at data type of each column to see what data types inferSchema=TRUE paramter has set for each column\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+---+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|age|\n",
      "+----+---------+------------+----------+------+------------+---+\n",
      "|18.0|        8|       307.0|       130|  3504|        12.0| 49|\n",
      "|15.0|        8|       350.0|       165|  3693|        11.5| 49|\n",
      "|18.0|        8|       318.0|       150|  3436|        11.0| 49|\n",
      "|16.0|        8|       304.0|       150|  3433|        12.0| 49|\n",
      "|17.0|        8|       302.0|       140|  3449|        10.5| 49|\n",
      "|15.0|        8|       429.0|       198|  4341|        10.0| 49|\n",
      "|14.0|        8|       454.0|       220|  4354|         9.0| 49|\n",
      "|14.0|        8|       440.0|       215|  4312|         8.5| 49|\n",
      "|14.0|        8|       455.0|       225|  4425|        10.0| 49|\n",
      "|15.0|        8|       390.0|       190|  3850|         8.5| 49|\n",
      "+----+---------+------------+----------+------+------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displaying first 10 rows of data\n",
    "\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Feature array by omitting the last column\n",
    "\n",
    "feature_columns = data.columns[:-1] \n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vect_assembler = VectorAssembler(inputCols=feature_columns,outputCol=\"inp_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilize Assembler created above in order to add the feature column to the original dataset\n",
    "\n",
    "data_features = vect_assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+---+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|age|        inp_features|\n",
      "+----+---------+------------+----------+------+------------+---+--------------------+\n",
      "|18.0|        8|       307.0|       130|  3504|        12.0| 49|[18.0,8.0,307.0,1...|\n",
      "|15.0|        8|       350.0|       165|  3693|        11.5| 49|[15.0,8.0,350.0,1...|\n",
      "|18.0|        8|       318.0|       150|  3436|        11.0| 49|[18.0,8.0,318.0,1...|\n",
      "|16.0|        8|       304.0|       150|  3433|        12.0| 49|[16.0,8.0,304.0,1...|\n",
      "|17.0|        8|       302.0|       140|  3449|        10.5| 49|[17.0,8.0,302.0,1...|\n",
      "|15.0|        8|       429.0|       198|  4341|        10.0| 49|[15.0,8.0,429.0,1...|\n",
      "|14.0|        8|       454.0|       220|  4354|         9.0| 49|[14.0,8.0,454.0,2...|\n",
      "|14.0|        8|       440.0|       215|  4312|         8.5| 49|[14.0,8.0,440.0,2...|\n",
      "|14.0|        8|       455.0|       225|  4425|        10.0| 49|[14.0,8.0,455.0,2...|\n",
      "|15.0|        8|       390.0|       190|  3850|         8.5| 49|[15.0,8.0,390.0,1...|\n",
      "|15.0|        8|       383.0|       170|  3563|        10.0| 49|[15.0,8.0,383.0,1...|\n",
      "|14.0|        8|       340.0|       160|  3609|         8.0| 49|[14.0,8.0,340.0,1...|\n",
      "|15.0|        8|       400.0|       150|  3761|         9.5| 49|[15.0,8.0,400.0,1...|\n",
      "|14.0|        8|       455.0|       225|  3086|        10.0| 49|[14.0,8.0,455.0,2...|\n",
      "|24.0|        4|       113.0|        95|  2372|        15.0| 49|[24.0,4.0,113.0,9...|\n",
      "|22.0|        6|       198.0|        95|  2833|        15.5| 49|[22.0,6.0,198.0,9...|\n",
      "|18.0|        6|       199.0|        97|  2774|        15.5| 49|[18.0,6.0,199.0,9...|\n",
      "|21.0|        6|       200.0|        85|  2587|        16.0| 49|[21.0,6.0,200.0,8...|\n",
      "|27.0|        4|        97.0|        88|  2130|        14.5| 49|[27.0,4.0,97.0,88...|\n",
      "|26.0|        4|        97.0|        46|  1835|        20.5| 49|[26.0,4.0,97.0,46...|\n",
      "+----+---------+------------+----------+------+------------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the data having additional column named features\n",
    "# independent feature values combined in one list and inserted in column as last cell.\n",
    "\n",
    "data_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|        inp_features|age|\n",
      "+--------------------+---+\n",
      "|[18.0,8.0,307.0,1...| 49|\n",
      "|[15.0,8.0,350.0,1...| 49|\n",
      "|[18.0,8.0,318.0,1...| 49|\n",
      "|[16.0,8.0,304.0,1...| 49|\n",
      "|[17.0,8.0,302.0,1...| 49|\n",
      "+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select only Features and Label from dataset\n",
    "#here label is age of vehicle\n",
    "\n",
    "finalized_dataset = data_features.select(\"inp_features\",\"age\")\n",
    "\n",
    "finalized_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting the data into training and testing with 80% data going in training and 20% in testing\n",
    "\n",
    "train_dataset, test_dataset = finalized_dataset .randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Linear Regression class as lr\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression as lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the lr object named having feature column as inp_features and Label column as age\n",
    "\n",
    "L_obj = lr(featuresCol=\"inp_features\", labelCol=\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model by using fit() method with lr object\n",
    "\n",
    "model = L_obj.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the age using the evulate method \n",
    "\n",
    "pred = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------------+\n",
      "|        inp_features|age|        prediction|\n",
      "+--------------------+---+------------------+\n",
      "|[9.0,8.0,304.0,19...| 49| 45.43928180396885|\n",
      "|[10.0,8.0,360.0,2...| 49| 46.38094468989763|\n",
      "|[11.0,8.0,318.0,2...| 49| 46.29940745229791|\n",
      "|[12.0,8.0,383.0,1...| 48|42.885233412020625|\n",
      "|[13.0,8.0,302.0,1...| 44| 48.71031554781278|\n",
      "+--------------------+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the predicted age values along side actual age values in the dataset\n",
    "\n",
    "pred.predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.77\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model using metric like  Root Mean Square Error(RMSE) and \n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluation = RegressionEvaluator(labelCol=\"age\", predictionCol=\"prediction\")\n",
    "\n",
    "# Root Mean Square Error\n",
    "\n",
    "rmse = evaluation.evaluate(pred.predictions, {evaluation.metricName: \"rmse\"})\n",
    "print(\"RMSE: %.2f\" % rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence there is much difference between predicted and actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopping the spark session\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
